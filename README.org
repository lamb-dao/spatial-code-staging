* eo-learn
** setup

#+begin_src python

  # Jupyter notebook related
  %reload_ext autoreload
  %autoreload 2
  %matplotlib inline

  import datetime
  import itertools

  # Built-in modules
  import os

  # Basics of Python data handling and visualization
  import numpy as np
  from aenum import MultiValueEnum

  np.random.seed(42)
  import geopandas as gpd
  import joblib

  # Machine learning
  import lightgbm as lgb
  import matplotlib.pyplot as plt
  from matplotlib.colors import BoundaryNorm, ListedColormap
  from shapely.geometry import Polygon
  from sklearn import metrics, preprocessing
  from tqdm.auto import tqdm

  from sentinelhub import DataCollection, UtmZoneSplitter

  # Imports from eo-learn and sentinelhub-py
  from eolearn.core import (
      EOExecutor,
      EOPatch,
      EOTask,
      EOWorkflow,
      FeatureType,
      LoadTask,
      MergeFeatureTask,
      OverwritePermission,
      SaveTask,
      linearly_connect_tasks,
  )
  from eolearn.features import NormalizedDifferenceIndexTask, SimpleFilterTask
  from eolearn.features.extra.interpolation import LinearInterpolationTask
  from eolearn.geometry import ErosionTask, VectorToRasterTask
  from eolearn.io import ExportToTiffTask, SentinelHubInputTask, VectorImportTask &&& ImportFromTiffTask
  from eolearn.ml_tools import FractionSamplingTask+begin_src python
#+end_src

** data load
*** parse layers gpkg
**** define aoi

#+begin_src python
#+end_src

#+begin_src python
  # Folder where data for running the notebook is stored
DATA_FOLDER = os.path.join("..", "..", "example_data")
# Locations for collected data and intermediate results
EOPATCH_FOLDER = os.path.join(".", "eopatches")
EOPATCH_SAMPLES_FOLDER = os.path.join(".", "eopatches_sampled")
RESULTS_FOLDER = os.path.join(".", "results")
for folder in (EOPATCH_FOLDER, EOPATCH_SAMPLES_FOLDER, RESULTS_FOLDER):
    os.makedirs(folder, exist_ok=True)

# Load geojson file
country = gpd.read_file(os.path.join(DATA_FOLDER, "svn_border.geojson"))
# Add 500m buffer to secure sufficient data near border
country = country.buffer(500)

# Get the country's shape in polygon format
country_shape = country.geometry.values[0]

# Plot country
country.plot()
plt.axis("off")

# Print size
country_width = country_shape.bounds[2] - country_shape.bounds[0]
country_height = country_shape.bounds[3] - country_shape.bounds[1]
print(f"Dimension of the area is {country_width:.0f} x {country_height:.0f} m2")
#+end_src
**** identify layers

#+begin_src python
&&& gpgk info to list and filter
#+end_src

*** eo-learn input task
**** load layers to patch

#+begin_src python
####Define custom EOTasks
class SentinelHubValidDataTask(EOTask):
    """
    Combine Sen2Cor's classification map with `IS_DATA` to define a `VALID_DATA_SH` mask
    The SentinelHub's cloud mask is asumed to be found in eopatch.mask['CLM']
    """

    def __init__(self, output_feature):
        self.output_feature = output_feature

    def execute(self, eopatch):
        eopatch[self.output_feature] = eopatch.mask["IS_DATA"].astype(bool) & (~eopatch.mask["CLM"].astype(bool))
        return eopatch


class AddValidCountTask(EOTask):
    """
    The task counts number of valid observations in time-series and stores the results in the timeless mask.
    """

    def __init__(self, count_what, feature_name):
        self.what = count_what
        self.name = feature_name

    def execute(self, eopatch):
        eopatch[FeatureType.MASK_TIMELESS, self.name] = np.count_nonzero(eopatch.mask[self.what], axis=0)
        return eopatch

#### Define the workflow tasks
# BAND DATA
# Add a request for S2 bands.
# Here we also do a simple filter of cloudy scenes (on tile level).
# The s2cloudless masks and probabilities are requested via additional data.
band_names = ["B02", "B03", "B04", "B08", "B11", "B12"]
add_data = SentinelHubInputTask(
    bands_feature=(FeatureType.DATA, "BANDS"),
    bands=band_names,
    resolution=10,
    maxcc=0.8,
    time_difference=datetime.timedelta(minutes=120),
    data_collection=DataCollection.SENTINEL2_L1C,
    additional_data=[(FeatureType.MASK, "dataMask", "IS_DATA"), (FeatureType.MASK, "CLM"), (FeatureType.DATA, "CLP")],
    max_threads=5,
)


# CALCULATING NEW FEATURES
# NDVI: (B08 - B04)/(B08 + B04)
# NDWI: (B03 - B08)/(B03 + B08)
# NDBI: (B11 - B08)/(B11 + B08)
ndvi = NormalizedDifferenceIndexTask(
    (FeatureType.DATA, "BANDS"), (FeatureType.DATA, "NDVI"), [band_names.index("B08"), band_names.index("B04")]
)
ndwi = NormalizedDifferenceIndexTask(
    (FeatureType.DATA, "BANDS"), (FeatureType.DATA, "NDWI"), [band_names.index("B03"), band_names.index("B08")]
)
ndbi = NormalizedDifferenceIndexTask(
    (FeatureType.DATA, "BANDS"), (FeatureType.DATA, "NDBI"), [band_names.index("B11"), band_names.index("B08")]
)


# VALIDITY MASK
# Validate pixels using SentinelHub's cloud detection mask and region of acquisition
add_sh_validmask = SentinelHubValidDataTask((FeatureType.MASK, "IS_VALID"))

# COUNTING VALID PIXELS
# Count the number of valid observations per pixel using valid data mask
add_valid_count = AddValidCountTask("IS_VALID", "VALID_COUNT")

# SAVING TO OUTPUT (if needed)
save = SaveTask(EOPATCH_FOLDER, overwrite_permission=OverwritePermission.OVERWRITE_FEATURES)+begin_src python
#+end_src

**** define timestamps

#+begin_src python
  &&& ensure timestamps from gpgk metadata are transferred
#+end_src

*** parse segment gpkg
**** create reference map task

the segment gpgk file contains polygons and their corresponding labels

Ensure compatibility with eolearn LULC rasterization
eg gpgk for 10 segments
lulcid = 0, name = no data
lulcid = 1, name = cultivated land
lulcid = 2, name = forest
lulcid = 3, name = grassland
lulcid = 4, name = shrubland
lulcid = 5, name = water
lulcid = 6, name = wetlands
lulcid = 7, name = tundra
lulcid = 8, name = artificial surface
lulcid = 9, name = bareland
lulcid = 10, name = snow and ice

eg
land cover enum definition
#+begin_src python
  class LULC(MultiValueEnum):
    """Enum class containing basic LULC types"""

    # &&& create text block programatically from gpgk parser
    NO_DATA = "No Data", 0, "#ffffff"
    CULTIVATED_LAND = "Cultivated Land", 1, "#ffff00"
    FOREST = "Forest", 2, "#054907"
    GRASSLAND = "Grassland", 3, "#ffa500"
    SHRUBLAND = "Shrubland", 4, "#806000"
    WATER = "Water", 5, "#069af3"
    WETLAND = "Wetlands", 6, "#95d0fc"
    TUNDRA = "Tundra", 7, "#967bb6"
    ARTIFICIAL_SURFACE = "Artificial Surface", 8, "#dc143c"
    BARELAND = "Bareland", 9, "#a6a6a6"
    SNOW_AND_ICE = "Snow and Ice", 10, "#000000"

    @property
    def id(self):
        return self.values[1]

    @property
    def color(self):
        return self.values[2]


# Reference colormap things
lulc_cmap = ListedColormap([x.color for x in LULC], name="lulc_cmap")
lulc_norm = BoundaryNorm([x - 0.5 for x in range(len(LULC) + 1)], lulc_cmap.N)
#+end_src

**** convert vector segments to raster

#+begin_src python
  land_use_ref_path = os.path.join(DATA_FOLDER, "land_use_10class_reference_slovenia_partial.gpkg")
  vector_feature = FeatureType.VECTOR_TIMELESS, "LULC_REFERENCE"

  vector_import_task = VectorImportTask(vector_feature, land_use_ref_path)

  rasterization_task = VectorToRasterTask(
      vector_feature,
      (FeatureType.MASK_TIMELESS, "LULC"),
      values_column="lulcid",
      raster_shape=(FeatureType.MASK, "IS_DATA"),
      raster_dtype=np.uint8,
  )
#+end_src

**** run workflow

#+begin_src python
  # Define the workflow
workflow_nodes = linearly_connect_tasks(
    add_data, ndvi, ndwi, ndbi, add_sh_validmask, add_valid_count, vector_import_task, rasterization_task, save
)
workflow = EOWorkflow(workflow_nodes)
# Time interval for the SH request
time_interval = ["2019-01-01", "2019-12-31"]

# Define additional parameters of the workflow
input_node = workflow_nodes[0]
save_node = workflow_nodes[-1]
execution_args = []
for idx, bbox in enumerate(bbox_list[patch_ids]):
    execution_args.append(
        {
            input_node: {"bbox": bbox, "time_interval": time_interval},
            save_node: {"eopatch_folder": f"eopatch_{idx}"},
        }
    )

# Execute the workflow
executor = EOExecutor(workflow, execution_args, save_logs=True)
executor.run(workers=4)

executor.make_report()

failed_ids = executor.get_failed_executions()
if failed_ids:
    raise RuntimeError(
        f"Execution failed EOPatches with IDs:\n{failed_ids}\n"
        f"For more info check report at {executor.get_report_path()}"
    )
#+end_src

*** verify loaded dataset

#+begin_src python
      # check data structure
          EOPatch.load('./eopatches/eopatch_0/')

          eopatch.timestamps
          eopatch.mask['LULC']
          eopatch.data['NDVI'][0]
          eopatch.data['BANDS'][5][..., [3, 2, 1]]


      # Draw the RGB images
        fig, axs = plt.subplots(nrows=5, ncols=5, figsize=(20, 20))

        date = datetime.datetime(2019, 7, 1)

        for i in tqdm(range(len(patch_ids))):
            eopatch_path = os.path.join(EOPATCH_FOLDER, f"eopatch_{i}")
            eopatch = EOPatch.load(eopatch_path, lazy_loading=True)

            dates = np.array([timestamp.replace(tzinfo=None) for timestamp in eopatch.timestamps])
            closest_date_id = np.argsort(abs(date - dates))[0]

            ax = axs[i // 5][i % 5]
            ax.imshow(np.clip(eopatch.data["BANDS"][closest_date_id][..., [2, 1, 0]] * 3.5, 0, 1))
            ax.set_xticks([])
            ax.set_yticks([])
            ax.set_aspect("auto")
            del eopatch

        fig.subplots_adjust(wspace=0, hspace=0)


      # Visualize the reference map
      fig, axs = plt.subplots(nrows=5, ncols=5, figsize=(20, 25))

      for i in tqdm(range(len(patch_ids))):
          eopatch_path = os.path.join(EOPATCH_FOLDER, f"eopatch_{i}")
          eopatch = EOPatch.load(eopatch_path, lazy_loading=True)

          ax = axs[i // 5][i % 5]
          im = ax.imshow(eopatch.mask_timeless["LULC"].squeeze(), cmap=lulc_cmap, norm=lulc_norm)
          ax.set_xticks([])
          ax.set_yticks([])
          ax.set_aspect("auto")
          del eopatch

      fig.subplots_adjust(wspace=0, hspace=0)

      cb = fig.colorbar(im, ax=axs.ravel().tolist(), orientation="horizontal", pad=0.01, aspect=100)
      cb.ax.tick_params(labelsize=20)
      cb.set_ticks([entry.id for entry in LULC])
      cb.ax.set_xticklabels([entry.name for entry in LULC], rotation=45, fontsize=15)
      plt.show();

    # Plot the map of valid pixel counts

    # Calculate min and max counts of valid data per pixel
    vmin, vmax = None, None
    for i in range(len(patch_ids)):
        eopatch_path = os.path.join(EOPATCH_FOLDER, f"eopatch_{i}")
        eopatch = EOPatch.load(eopatch_path, lazy_loading=True)
        data = eopatch.mask_timeless["VALID_COUNT"].squeeze()
        vmin = np.min(data) if vmin is None else (np.min(data) if np.min(data) < vmin else vmin)
        vmax = np.max(data) if vmax is None else (np.max(data) if np.max(data) > vmax else vmax)

    fig, axs = plt.subplots(nrows=5, ncols=5, figsize=(20, 25))

    for i in tqdm(range(len(patch_ids))):
        eopatch_path = os.path.join(EOPATCH_FOLDER, f"eopatch_{i}")
        eopatch = EOPatch.load(eopatch_path, lazy_loading=True)
        ax = axs[i // 5][i % 5]
        im = ax.imshow(eopatch.mask_timeless["VALID_COUNT"].squeeze(), vmin=vmin, vmax=vmax, cmap=plt.cm.inferno)
        ax.set_xticks([])
        ax.set_yticks([])
        ax.set_aspect("auto")
        del eopatch

    fig.subplots_adjust(wspace=0, hspace=0)

    cb = fig.colorbar(im, ax=axs.ravel().tolist(), orientation="horizontal", pad=0.01, aspect=100)
    cb.ax.tick_params(labelsize=20)
    plt.show()


  # Spatial mean of NDVI
  Plot the mean of NDVI over all pixels in a selected patch throughout the year. Filter out clouds in the mean calculation.

  eopatch = EOPatch.load(os.path.join(EOPATCH_FOLDER, f"eopatch_{i}"), lazy_loading=True)

  ndvi = eopatch.data["NDVI"]
  mask = eopatch.mask["IS_VALID"]
  time = np.array(eopatch.timestamps)
  t, w, h, _ = ndvi.shape

  ndvi_clean = ndvi.copy()
  ndvi_clean[~mask] = np.nan  # Set values of invalid pixels to NaN's

  # Calculate means, remove NaN's from means
  ndvi_mean = np.nanmean(ndvi.reshape(t, w * h), axis=1)
  ndvi_mean_clean = np.nanmean(ndvi_clean.reshape(t, w * h), axis=1)
  time_clean = time[~np.isnan(ndvi_mean_clean)]
  ndvi_mean_clean = ndvi_mean_clean[~np.isnan(ndvi_mean_clean)]

  fig = plt.figure(figsize=(20, 5))
  plt.plot(time_clean, ndvi_mean_clean, "s-", label="Mean NDVI with cloud cleaning")
  plt.plot(time, ndvi_mean, "o-", label="Mean NDVI without cloud cleaning")
  plt.xlabel("Time", fontsize=15)
  plt.ylabel("Mean NDVI over patch", fontsize=15)
  plt.xticks(fontsize=15)
  plt.yticks(fontsize=15)

  plt.legend(loc=2, prop={"size": 15});

# Temporal mean of NDVI
Plot the time-wise mean of NDVI for the whole region. Filter out clouds in the mean calculation.

fig, axs = plt.subplots(nrows=5, ncols=5, figsize=(20, 25))

for i in tqdm(range(len(patch_ids))):
    eopatch_path = os.path.join(EOPATCH_FOLDER, f"eopatch_{i}")
    eopatch = EOPatch.load(eopatch_path, lazy_loading=True)
    ndvi = eopatch.data["NDVI"]
    mask = eopatch.mask["IS_VALID"]
    ndvi[~mask] = np.nan
    ndvi_mean = np.nanmean(ndvi, axis=0).squeeze()

    ax = axs[i // 5][i % 5]
    im = ax.imshow(ndvi_mean, vmin=0, vmax=0.8, cmap=plt.get_cmap("YlGn"))
    ax.set_xticks([])
    ax.set_yticks([])
    ax.set_aspect("auto")
    del eopatch

fig.subplots_adjust(wspace=0, hspace=0)

cb = fig.colorbar(im, ax=axs.ravel().tolist(), orientation="horizontal", pad=0.01, aspect=100)
cb.ax.tick_params(labelsize=20)
plt.show()
#+end_src

** data clean
*** filtering

#+begin_src python
  # LOAD EXISTING EOPATCHES
  load = LoadTask(EOPATCH_FOLDER)

  # FEATURE CONCATENATION
  concatenate = MergeFeatureTask({FeatureType.DATA: ["BANDS", "NDVI", "NDWI", "NDBI"]}, (FeatureType.DATA, "FEATURES"))

  # FILTER OUT CLOUDY SCENES

  class ValidDataFractionPredicate:
       """Predicate that defines if a frame from EOPatch's time-series is valid or not. Frame is valid if the
      valid data fraction is above the specified threshold.
      """

      def __init__(self, threshold):
          self.threshold = threshold

      def __call__(self, array):
          coverage = np.sum(array.astype(np.uint8)) / np.prod(array.shape)
          return coverage > self.threshold

  # Keep frames with > 80% valid coverage
  valid_data_predicate = ValidDataFractionPredicate(0.8)
  filter_task = SimpleFilterTask((FeatureType.MASK, "IS_VALID"), valid_data_predicate)
#+end_src

*** temporal gap filling

#+begin_src python
  # LINEAR TEMPORAL INTERPOLATION
  # linear interpolation of full time-series and date resampling
  resampled_range = ("2019-01-01", "2019-12-31", 15)
  linear_interp = LinearInterpolationTask(
      (FeatureType.DATA, "FEATURES"),  # name of field to interpolate
      mask_feature=(FeatureType.MASK, "IS_VALID"),  # mask to be used in interpolation
      copy_features=[(FeatureType.MASK_TIMELESS, "LULC")],  # features to keep
      resample_range=resampled_range,
  )
#+end_src

*** noise erosion

#+begin_src python
  # EROSION
  # erode each class of the reference map
  erosion = ErosionTask(mask_feature=(FeatureType.MASK_TIMELESS, "LULC", "LULC_ERODED"), disk_radius=1)
#+end_src

*** spatial sampling

#+begin_src python
  # SPATIAL SAMPLING
  # Uniformly sample pixels from patches
  lulc_type_ids = [lulc_type.id for lulc_type in LULC]

  spatial_sampling = FractionSamplingTask(
      features_to_sample=[(FeatureType.DATA, "FEATURES", "FEATURES_SAMPLED"), (FeatureType.MASK_TIMELESS, "LULC_ERODED")],
      sampling_feature=(FeatureType.MASK_TIMELESS, "LULC_ERODED"),
      fraction=0.25,  # a quarter of points
      exclude_values=[0],
  )
#+end_src

*** run workflow

#+begin_src python
  save = SaveTask(EOPATCH_SAMPLES_FOLDER, overwrite_permission=OverwritePermission.OVERWRITE_FEATURES)
  # Define the workflow
  workflow_nodes = linearly_connect_tasks(load, concatenate, filter_task, linear_interp, erosion, spatial_sampling, save)
  workflow = EOWorkflow(workflow_nodes)
  Run the EOWorkflow over all EOPatches
  %%time

  execution_args = []
  for idx in range(len(patch_ids)):
      execution_args.append(
          {
              workflow_nodes[0]: {"eopatch_folder": f"eopatch_{idx}"},  # load
              workflow_nodes[-2]: {"seed": 42},  # sampling
              workflow_nodes[-1]: {"eopatch_folder": f"eopatch_{idx}"},  # save
          }
      )

  executor = EOExecutor(workflow, execution_args, save_logs=True)
  executor.run(workers=5)

  executor.make_report()

  failed_ids = executor.get_failed_executions()
  if failed_ids:
      raise RuntimeError(
          f"Execution failed EOPatches with IDs:\n{failed_ids}\n"
          f"For more info check report at {executor.get_report_path()}"
      )
#+end_src

** construct and train model
*** train test split

#+begin_src python
   Load sampled eopatches
sampled_eopatches = []

for i in range(len(patch_ids)):
    sample_path = os.path.join(EOPATCH_SAMPLES_FOLDER, f"eopatch_{i}")
    sampled_eopatches.append(EOPatch.load(sample_path, lazy_loading=True))
# Definition of the train and test patch IDs, take 80 % for train
test_ids = [0, 8, 16, 19, 20]
test_eopatches = [sampled_eopatches[i] for i in test_ids]
train_ids = [i for i in range(len(patch_ids)) if i not in test_ids]
train_eopatches = [sampled_eopatches[i] for i in train_ids]

# Set the features and the labels for train and test sets
features_train = np.concatenate([eopatch.data["FEATURES_SAMPLED"] for eopatch in train_eopatches], axis=1)
labels_train = np.concatenate([eopatch.mask_timeless["LULC_ERODED"] for eopatch in train_eopatches], axis=0)

features_test = np.concatenate([eopatch.data["FEATURES_SAMPLED"] for eopatch in test_eopatches], axis=1)
labels_test = np.concatenate([eopatch.mask_timeless["LULC_ERODED"] for eopatch in test_eopatches], axis=0)

# Get shape
t, w1, h, f = features_train.shape
t, w2, h, f = features_test.shape

# Reshape to n x m
features_train = np.moveaxis(features_train, 0, 2).reshape(w1 * h, t * f)
labels_train = labels_train.reshape(w1 * h)
features_test = np.moveaxis(features_test, 0, 2).reshape(w2 * h, t * f)
labels_test = labels_test.reshape(w2 * h)
features_train.shape
#+end_src

*** Train

#+begin_src python
    # Set up training classes
  labels_unique = np.unique(labels_train)

  # Set up the model
  model = lgb.LGBMClassifier(
      objective="multiclass", num_class=len(labels_unique), metric="multi_logloss", random_state=42
  )

  # Train the model
  model.fit(features_train, labels_train)

  # Save the model
  joblib.dump(model, os.path.join(RESULTS_FOLDER, "model_SI_LULC.pkl"))
#+end_src

** validate model

#+begin_src python
#+end_src

** Visualizations

#+begin_src python
#+end_src

* Frameworks
** eo learn

- https://eo-learn.readthedocs.io/en/latest/
- https://github.com/sentinel-hub/eo-learn
- https://github.com/sentinel-hub/eo-learn-examples/tree/main
- https://github.com/sentinel-hub/eo-learn/tree/master/examples

** tsai

- https://timeseriesai.github.io/tsai/
- https://github.com/timeseriesAI/tsai

** fastai

- https://docs.fast.ai/
- https://github.com/fastai/fastai

** sklearn

- https://scikit-learn.org/stable/

** pytorch

- https://pytorch.org/
- https://github.com/pytorch/pytorch
- https://pytorch.org/docs/stable/index.html

- dataloader and forward method
  - Datapipes
  - https://www.learnpytorch.io/00_pytorch_fundamentals/
  - https://github.com/mrdbourke/pytorch-deep-learning/

* Resources

comprehensive VAE with tensorboard
https://hunterheidenreich.com/posts/modern-variational-autoencoder-in-pytorch/

detailed implement then sample/visualize a t model
https://towardsdatascience.com/a-complete-guide-to-write-your-own-transformers-29e23f371ddd

implemented collection of vae in pytorch
https://github.com/AntixK/PyTorch-VAE

concepts from autoencoder to BVAE
https://lilianweng.github.io/posts/2018-08-12-vae/

n dim serial
https://github.com/jonzia/Recurrent_Autoencoder

https://github.com/RobRomijnders/AE_ts
https://github.com/RobRomijnders/ts_clust/tree/master/ts_clust

* Removed
** Architecture
*** B-VAE

- Theory https://lilianweng.github.io/posts/2018-08-12-vae/
- Disentangelment experiments https://wandb.ai/arpastrana/beta_vae/reports/Disentangling-Variational-Autoencoders--VmlldzozNDQ3MDk
- 2 stage training for disentanglement https://arxiv.org/abs/2209.14783
- Implemented with solver https://github.com/AxelNathanson/pytorch-Variational-Autoencoder
- Sne vae clustering https://github.com/jgraving/selfsne
- Vae from scratch https://m.youtube.com/watch?v=VELQT1-hILo

*** byT5

- Architecture video https://m.youtube.com/watch?v=bCz4OMemCcA
- [ACTOR transformer vae](https://github.com/Mathux/ACTOR)
- hackable Gpt trainer https://github.com/karpathy/nanoGPT
- Train/finetune gpt1 https://github.com/akshat0123/GPT-1

** Data loader

- overview
  - include dates in lispy text based ip:op string, minimal character inclusion
    eg (ip (date (red (gaussian stack)) (green (gaussian stack)) (blue (gaussian stack) )) op ((dia) (weight) (resistance) (normed-weight)))
- byT5 data prep
  - T5 architecture with B-VAE inserted
    - T5 trainer scripts and utilities

- Write dataloader
	- Tools
    - ruricolist/random-sample
    - mito to SQlite
    - py4cl cmd
    - lparallel
    - mgl dataloader
    - memoization (modify to add to mito)
	- less likely Tools
		- Dask
		- Geopandas
		- Daskgeopandas
		- xarray

	- Input file structure
    - control dir
      - AOI extent
      - Train test extent
      - validation extent
      - Id num masks
      - replicate table
    - maps dir
      - EOLearn structure
      - Maps by sample date
    - predictions dir
      - Id key info

	- Out
		- Combined data vector normed
		- A observations
		- B prompt
		- C loss target
	- Targets
		- Clever meerkat, data panels

Prediction Target
	Swapable output training target, from field data, keyed to id num
	Validation automation
		Replicates and folds
		Map -> arc -> autoencoder -> prediction vs ground truth

- plist data interchange format

data collection
  ((data-seq . (<>))(predictions . (<>)))
    (data-seq . ((data-pt . (<>))...(data-pt . (<>))))
      (data-pt . ((loc . (<>))(blue . (<>))(green . (<>))(red . (<>))(nir . (<>))(ir . (<>))(ndri . (<>))(ndvi . (<>))))
        (loc . ((x . <LAT>)(y . <LON>)(t . <YYYY-MM-DD>)))
        (blue . ((3mm . <VAL>)(1cm . <VAL>)(5cm . <VAL>)(50cm . <VAL>)))
    (predictions . ((height-cm . <VAL>)...(dia-mm . <VAL>)))

prompt:
  ((data-seq . (<SPAN-CORRUPTED-SEQ>))(task . <PREDICTIONS-MEMBER>)
    span corrupt with <!>
  seq to seq pretraining
  seq to value prediction

target:
  (predictions-member. result)

Finally to text document such that pytorch data loader is satisfied

** Data Aggregator
sql queries over gpkg
map pixels into data unformatted dat at every location withing selected polygon

requires spatialite extension
mito syntax

#+begin_src python
  import sqlite3
  import rasterio

  def connect_spatialite(db_path):
      """Connect to SpatiaLite database"""
      conn = sqlite3.connect(db_path)
      conn.enable_load_extension(True)
      conn.load_extension('mod_spatialite')
      return conn

  def get_polygon_by_id(conn, table_name, polygon_id):
      """Retrieve a specific polygon by its ID"""
      cursor = conn.cursor()
      cursor.execute(f"""
          SELECT id, ST_AsText(geometry) as geom
          FROM {table_name}
          WHERE id = ?
      """, (polygon_id,))
      return cursor.fetchone()

  def get_pixels_in_polygon(conn, raster_grid_table, polygon_geom):
      """Get pixel locations within a given polygon"""
      cursor = conn.cursor()
      cursor.execute(f"""
          SELECT
              ST_X(ST_Centroid(geometry)) as x,
              ST_Y(ST_Centroid(geometry)) as y
          FROM {raster_grid_table}
          WHERE ST_Intersects(geometry, ST_GeomFromText(?))
      """, (polygon_geom,))
      return cursor.fetchall()

  def extract_raster_values_at_point(conn, rasters_table, x, y, srid=4326):
      """Extract pixel values from all rasters at a specific point"""
      cursor = conn.cursor()
      cursor.execute(f"""
          SELECT
              raster_name,
              ST_Value(raster, ST_GeomFromText('POINT(? ?)', ?)) as pixel_value
          FROM {rasters_table}
      """, (x, y, srid))
      return cursor.fetchall()

  def main(db_path):
      conn = connect_spatialite(db_path)

      try:
          # Get a specific polygon
          polygon = get_polygon_by_id(conn, 'my_polygons', 1)
          print("Polygon:", polygon)

          # Get pixels within that polygon
          pixels = get_pixels_in_polygon(conn, 'raster_grid', polygon[1])
          print("Pixels in Polygon:", pixels)

          # If we have a specific pixel location
          if pixels:
              x, y = pixels[0]
              raster_values = extract_raster_values_at_point(conn, 'rasters', x, y)
              print("Raster Values:", raster_values)

      finally:
          conn.close()

  if __name__ == '__main__':
      main('/path/to/spatialite.db') #compatible with gpkg
#+end_src

Alist data format using :keywords
#+begin_src lisp
;; Creating nested alists for sequence data points with results
(let ((sequences
      `((:sequence-1 . ((:datapoint . ((:time . "2024-01-01T10:00:00")
                                      (:x . 1.2)
                                      (:y . 3.4)
                                      (:z . 0.5)
                                      (:results . ((:value1 . 42.3)
                                                 (:value2 . 18.7)
                                                 (:value3 . 33.1)))))
                       (:datapoint . ((:time . "2024-01-01T10:00:01")
                                      (:x . 1.3)
                                      (:y . 3.5)
                                      (:z . 0.6)
                                      (:results . ((:value1 . 43.1)
                                                 (:value2 . 19.2)
                                                 (:value3 . 34.0)))))))
        (:sequence-2 . ((:datapoint . ((:time . "2024-01-01T10:00:00")
                                      (:x . 2.1)
                                      (:y . 4.2)
                                      (:z . 1.1)
                                      (:results . ((:value1 . 55.4)
                                                 (:value2 . 22.3)
                                                 (:value3 . 44.7)))))))))

 ;; Access specific values
 (let* ((seq1 (cdr (assoc :sequence-1 sequences)))
        (first-point (cdr (assoc :datapoint seq1)))
        (results (cdr (assoc :results first-point))))
   (cdr (assoc :value1 results)))  ; => 42.3

 ;; Function to extract all x values from a sequence
 (defun get-x-values (sequence-data)
   (mapcar #'(lambda (point)
               (cdr (assoc :x (cdr point))))
           (remove-if-not #'(lambda (pair)
                             (eq (car pair) :datapoint))
                         sequence-data)))

 ;; Get x values from sequence-1
 (get-x-values (cdr (assoc :sequence-1 sequences)))  ; => (1.2 1.3)

 ;; Function to get all value1 results from a sequence
 (defun get-value1-series (sequence-data)
   (mapcar #'(lambda (point)
               (let ((results (cdr (assoc :results (cdr point)))))
                 (cdr (assoc :value1 results))))
           (remove-if-not #'(lambda (pair)
                             (eq (car pair) :datapoint))
                         sequence-data)))

 ;; Calculate average of value1 for sequence-1
 (let ((values (get-value1-series (cdr (assoc :sequence-1 sequences)))))
   (/ (reduce #'+ values) (length values)))  ; => 42.7

 ;; Function to get all datapoints at a specific time
 (defun get-points-at-time (sequences time)
   (loop for (seq-name . seq-data) in sequences
         collect (cons seq-name
                      (find-if #'(lambda (point)
                                  (string= (cdr (assoc :time (cdr point))) time))
                              seq-data
                              :key #'car)))))

(get-points-at-time sequences "2024-01-01T10:00:00")

#+end_src

** Export to geopackage database

targeting gpkg spatial database formatting with tabular data
https://gdal.org/user/ogr_sql_dialect.html#joins
#+begin_src bash
  # Add polygon shapefile
  # &&& to single shape gpkg ip
  # at this point multiple polygons are flattened
  ogr2ogr -f GPKG -update output.gpkg input_polygons.shp -nln extents

  # join csv to shapefile
  ogr2ogr -sql "SELECT inshape.*, joincsv.* \
      FROM inshape \
      LEFT JOIN 'joincsv.csv'.joincsv \
      ON inshape.GISJOIN = joincsv.GISJOIN" \
          shape_join.shp inshape.shp
#+end_src

ogr2ogr to incrementally add geopackage layers
https://gdal.org/en/stable/programs/ogr2ogr.html
#+begin_src bash
  # car init raster
  ogr2ogr -om RASTER_DATE=YYYY-MM-DD -of GPKG output.gpkg first_raster.tif -nln raster-1

  # mapcar cadr append raster
  ogr2ogr -om RASTER_DATE=YYYY-MM-DD -of GPKG -update output.gpkg second_raster.tif -nln raster-2
  ogr2ogr -om RASTER_DATE=YYYY-MM-DD -of GPKG -update output.gpkg third_raster.tif -nln raster-3
#+end_src
#+begin_src lisp
  #layernames in a gpkg
  ogrinfo ~/qgis/AOI-buffered.gpkg

#+end_src
** Model ByT5 in pytorch
*** Data Loader

parallel text format in train.txt
#+begin_src
source_sentence_1 ||| target_sentence_1
source_sentence_2 ||| target_sentence_2
source_sentence_3 ||| target_sentence_3
#+end_src

#+begin_src python
import torch
from torch.utils.data import Dataset
import pandas as pd

class Seq2SeqDataset(Dataset):
    def __init__(self, file_path, source_tokenizer, target_tokenizer, max_length=128):
        # Read the data
        self.data = pd.read_csv(file_path, sep='|||', header=None, names=['source', 'target'])

        # Tokenize and encode
        self.source_tokens = [
            source_tokenizer.encode(
                text,
                max_length=max_length,
                truncation=True,
                padding='max_length'
            ) for text in self.data['source']
        ]

        self.target_tokens = [
            target_tokenizer.encode(
                text,
                max_length=max_length,
                truncation=True,
                padding='max_length'
            ) for text in self.data['target']
        ]

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        return {
            'source_ids': torch.tensor(self.source_tokens[idx], dtype=torch.long),
            'target_ids': torch.tensor(self.target_tokens[idx], dtype=torch.long)
        }
#+end_src

*** Span corruption pretraining objective
calculate spans method and apply to a pretraining text string
#+begin_src python
  def corrupt_spans(text: str, mean_span_length: int = 20, corruption_rate: float = 0.15):
      # Convert text to bytes
      byte_sequence = text.encode('utf-8')
      sequence_length = len(byte_sequence)

      # Calculate number of spans to corrupt
      target_corrupt_bytes = int(sequence_length * corruption_rate)
      spans = []
      current_corrupt_bytes = 0

      while current_corrupt_bytes < target_corrupt_bytes:
          # Sample span length from geometric distribution
          span_length = np.random.geometric(1/mean_span_length)

          # Sample start position
          valid_starts = sequence_length - span_length
          if valid_starts <= 0:
              break
          start = np.random.randint(0, valid_starts)

          spans.append((start, start + span_length))
          current_corrupt_bytes += span_length

      return spans

  def create_training_example(text: str, spans: List[Tuple[int, int]]):
      byte_sequence = text.encode('utf-8')
      corrupted = bytearray(byte_sequence)
      targets = []

      # Replace spans with sentinel tokens and collect targets
      for idx, (start, end) in enumerate(spans):
          sentinel = f"<X{idx}>".encode('utf-8')
          target = byte_sequence[start:end]
          corrupted[start:end] = sentinel
          targets.append((sentinel, target))

      return corrupted, targets


  def compute_span_loss(original_bytes, predicted_bytes, spans):
      loss = 0
    for span_start, span_end in spans:
        target = original_bytes[span_start:span_end]
        prediction = predicted_bytes[span_start:span_end]
        loss += cross_entropy(target, prediction)
    return loss / len(spans)



  def prepare_input(text, task_prefix=""):
    if task_prefix:
        full_input = f"{task_prefix}: {text}"
    else:
        full_input = text
        # Convert to bytes for model input
    return full_input.encode('utf-8')

  def prepare_target(text):
      # For pre-training, only include corrupted spans
      # For fine-tuning, include full target text
    return text.encode('utf-8')
#+end_src

Span corruption dataset integration
#+begin_src python
import os
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import numpy as np
import random
from transformers import PreTrainedTokenizerFast

class SpanCorruptionDataset(Dataset):
    def __init__(self, file_path, tokenizer, max_length=512, corruption_rate=0.15, mean_span_length=20):
        """
        Dataset for span corruption pre-training

        Args:
            file_path (str): Path to input text file
            tokenizer (PreTrainedTokenizerFast): Tokenizer for processing
            max_length (int): Maximum sequence length
            corruption_rate (float): Proportion of bytes to corrupt
            mean_span_length (int): Average length of corrupted spans
        """
        self.tokenizer = tokenizer
        self.max_length = max_length
        self.corruption_rate = corruption_rate
        self.mean_span_length = mean_span_length

        # Read text data
        with open(file_path, 'r', encoding='utf-8') as f:
            self.texts = [line.strip() for line in f if line.strip()]

    def _corrupt_spans(self, byte_sequence):
        """
        Corrupt spans in the byte sequence

        Args:
            byte_sequence (bytes): Input byte sequence

        Returns:
            tuple: (corrupted_sequence, original_spans)
        """
        sequence_length = len(byte_sequence)
        target_corrupt_bytes = int(sequence_length * self.corruption_rate)

        # Convert to bytearray for modification
        corrupted = bytearray(byte_sequence)
        spans = []
        current_corrupt_bytes = 0

        while current_corrupt_bytes < target_corrupt_bytes:
            # Sample span length from geometric distribution
            span_length = max(1, np.random.geometric(1/self.mean_span_length))

            # Ensure we don't exceed sequence length
            if span_length + current_corrupt_bytes > target_corrupt_bytes:
                span_length = target_corrupt_bytes - current_corrupt_bytes

            # Sample start position
            valid_starts = sequence_length - span_length
            if valid_starts <= 0:
                break

            start = np.random.randint(0, valid_starts)

            # Create sentinel token
            sentinel = f"<X{len(spans)}>".encode('utf-8')

            # Replace span with sentinel
            corrupted[start:start+span_length] = sentinel

            # Store original span and its position
            spans.append((start, start+span_length, byte_sequence[start:start+span_length]))

            current_corrupt_bytes += span_length

        return bytes(corrupted), spans

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, idx):
        # Encode text to bytes
        text_bytes = self.texts[idx].encode('utf-8')

        # Truncate to max length
        text_bytes = text_bytes[:self.max_length]

        # Perform span corruption
        corrupted_bytes, spans = self._corrupt_spans(text_bytes)

        # Prepare targets (only corrupted spans)
        targets = [span[2] for span in spans]
        target_indices = [span[0] for span in spans]

        return {
            'input_bytes': corrupted_bytes,
            'targets': targets,
            'target_indices': target_indices
        }


  def train(model, dataloader, optimizer, criterion, device, epochs=10):
      """
      Training loop for span corruption pre-training

      Args:
          model (ByT5Model): Model to train
          dataloader (DataLoader): Data loader with corrupted spans
          optimizer (torch.optim.Optimizer): Optimization algorithm
          criterion (nn.Module): Loss function
          device (torch.device): Training device
          epochs (int): Number of training epochs
      """
      model.train()

      for epoch in range(epochs):
          total_loss = 0

          for batch in dataloader:
              # Move data to device
              input_bytes = torch.tensor(np.frombuffer(batch['input_bytes'], dtype=np.uint8)).to(device)

              # Zero gradients
              optimizer.zero_grad()

              # Forward pass
              outputs = model(input_bytes)

              # Compute loss only for corrupted spans
              loss = 0
              for target, idx in zip(batch['targets'], batch['target_indices']):
                  target_bytes = torch.tensor(np.frombuffer(target, dtype=np.uint8)).to(device)
                  span_output = outputs[idx:idx+len(target_bytes)]

                  # Cross-entropy loss for span reconstruction
                  loss += criterion(span_output, target_bytes)

              # Backpropagate
              loss.backward()
              optimizer.step()

              total_loss += loss.item()

          print(f"Epoch {epoch+1}/{epochs}, Loss: {total_loss/len(dataloader)}")

  def main():
      """
      Main training script for ByT5 span corruption pre-training
      """
      # Set random seeds for reproducibility
      torch.manual_seed(42)
      np.random.seed(42)
      random.seed(42)

      # Device configuration
      device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

      # Instantiate model
      model = ByT5Model().to(device)

      # Create dummy tokenizer (for demonstration)
      class DummyTokenizer:
          def encode(self, text):
              return list(text.encode('utf-8'))

      # Create dataset and dataloader
      dataset = SpanCorruptionDataset(
          file_path='training_data.txt',  # Replace with your text file path
          tokenizer=DummyTokenizer(),
          max_length=512,
          corruption_rate=0.15
      )

      dataloader = DataLoader(
          dataset,
          batch_size=32,
          shuffle=True,
          num_workers=4
      )

      # Loss and optimizer
      criterion = nn.CrossEntropyLoss()
      optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)

      # Train the model
      train(
          model=model,
          dataloader=dataloader,
          optimizer=optimizer,
          criterion=criterion,
          device=device,
          epochs=10
      )

      # Save the model
      torch.save(model.state_dict(), 'byt5_model.pth')

  if __name__ == '__main__':
      main()
#+end_src
*** Pre training Tokenizer
#+begin_src python
  from torch.utils.data import Dataset, DataLoader
  import torch
  import numpy as np
  from dataclasses import dataclass
  from typing import List, Tuple
  import random

          @dataclass
          class SpanCorruptionConfig:
              mean_span_length: int = 3
              corruption_rate: float = 0.15
              max_span_length: int = 10

          class ByT5Style:
              # Special token IDs (we add these after the 256 ASCII bytes)
              PAD_ID = 256
              EOS_ID = 257
              UNK_ID = 258
              # Start sentinel tokens from 259 onwards
              SENTINEL_START = 259
              SENTINEL_END = 269  # Supporting up to 10 sentinel tokens

              VOCAB_SIZE = SENTINEL_END + 1

          class ByT5Dataset(Dataset):
              def __init__(
                      self,
                      file_path: str,
                      seq_length: int = 512,
                      stride: int = None,
                      span_corruption_config: SpanCorruptionConfig = None
              ):

                  self.seq_length = seq_length
                  self.stride = stride if stride else seq_length
                  self.span_corruption_config = span_corruption_config or SpanCorruptionConfig()

                  # Read all text as ASCII bytes
                  with open(file_path, 'r', encoding='ascii') as f:
                      self.data = f.read().encode('ascii')

                  # Split into lines and process lines directly
                  self.lines = [line.encode('ascii') for line in
                                open(file_path, 'r', encoding='ascii').readlines()]

                  # Calculate number of sequences
                  self.n_sequences = sum(
                      max(1, (len(line) - self.seq_length) // self.seq_length + 1)
                      for line in self.lines
                  )

              def _get_random_spans(self, length: int) -> List[Tuple[int, int]]:
                  """Generate random spans for corruption."""
                  target_corrupted = int(length * self.span_corruption_config.corruption_rate)
                  corrupted = 0
                  spans = []

                  while corrupted < target_corrupted:
                      # Sample span length from geometric distribution
                      span_length = min(
                          np.random.geometric(1 / self.span_corruption_config.mean_span_length),
                          self.span_corruption_config.max_span_length
                      )

                      # Ensure we don't corrupt too much
                      if corrupted + span_length > target_corrupted:
                          span_length = target_corrupted - corrupted

                      # Random start position
                      available_positions = length - span_length
                      if available_positions <= 0:
                          break

                      start = random.randint(0, available_positions)
                      spans.append((start, start + span_length))
                      corrupted += span_length

                  return sorted(spans)

              def _apply_span_corruption(
                      self,
                      sequence: bytes
              ) -> Tuple[torch.Tensor, torch.Tensor]:
                  """Apply span corruption to create input and target sequences."""
                  spans = self._get_random_spans(len(sequence))

                  # Create input sequence with sentinel tokens
                  input_ids = []
                  target_ids = []
                  last_position = 0
                  sentinel_idx = 0

                  for start, end in spans:
                      # Copy unchanged tokens
                      input_ids.extend(sequence[last_position:start])

                      # Add sentinel token to input
                      sentinel_token = ByT5Style.SENTINEL_START + sentinel_idx
                      input_ids.append(sentinel_token)

                      # Add corrupted span to target with sentinel token
                      target_ids.append(sentinel_token)
                      target_ids.extend(sequence[start:end])

                      last_position = end
                      sentinel_idx = (sentinel_idx + 1) % (ByT5Style.SENTINEL_END - ByT5Style.SENTINEL_START)

                  # Add remaining tokens
                  input_ids.extend(sequence[last_position:])

                  # Pad sequences to desired length
                  input_ids = input_ids[:self.seq_length]
                  input_ids.extend([ByT5Style.PAD_ID] * (self.seq_length - len(input_ids)))

                  target_ids = target_ids[:self.seq_length]
                  target_ids.extend([ByT5Style.PAD_ID] * (self.seq_length - len(target_ids)))

                  return (
                      torch.tensor(input_ids, dtype=torch.long),
                      torch.tensor(target_ids, dtype=torch.long)
                  )

              def __len__(self):
                  return self.n_sequences

                def __getitem__(self, idx):
                    # Iterate through lines to find the right sequence
                  cumulative_idx = 0
                  for line in self.lines:
                      # Determine how many sequences this line will generate
                      line_sequences = max(1, (len(line) - self.seq_length) // self.seq_length + 1)

                      if idx < cumulative_idx + line_sequences:
                          # Found the right line
                          local_idx = idx - cumulative_idx

                          # Handle different line length scenarios
                          if len(line) <= self.seq_length:
                              # Short line: pad to full sequence length
                              sequence = line + b'\x00' * (self.seq_length - len(line))
                          else:
                              # Long line: extract specific subsequence
                              start_pos = local_idx * self.seq_length
                              sequence = line[start_pos:start_pos + self.seq_length]

                              # Pad if the extracted sequence is too short
                              if len(sequence) < self.seq_length:
                                  sequence = sequence + b'\x00' * (self.seq_length - len(sequence))

                          # Apply span corruption
                          input_ids, target_ids = self._apply_span_corruption(sequence)

                          return {
                              'input_ids': input_ids,
                              'target_ids': target_ids
                          }

                      cumulative_idx += line_sequences

                raise IndexError("Sequence index out of range")


          def create_byt5_dataloader(
                  file_path: str,
                  batch_size: int = 32,
                  seq_length: int = 512,
                  span_corruption_config: SpanCorruptionConfig = None
          ):
              """Create a DataLoader with ByT5-style tokenization and span corruption."""
              dataset = ByT5Dataset(
                  file_path,
                  seq_length=seq_length,
                  span_corruption_config=span_corruption_config
              )

              return DataLoader(
                  dataset,
                  batch_size=batch_size,
                  shuffle=True,
                  num_workers=4
              ), ByT5Style.VOCAB_SIZE


          # Example usage:
        def main():
            config = SpanCorruptionConfig(
                mean_span_length=3,
                corruption_rate=0.15,
                max_span_length=10
            )

              dataloader, vocab_size = create_byt5_dataloader(
                  'your_text_file.txt',
                  span_corruption_config=config
              )

              # First batch
              batch = next(iter(dataloader))
              print(f"Input shape: {batch['input_ids'].shape}")
              print(f"Target shape: {batch['target_ids'].shape}")

          if __name__ == "__main__":
              main()
#+end_src
*** Architecture
**** b VAE

#+begin_src python
import torch
import torch.nn as nn
from torch.utils.data import DataLoader
from torchvision import datasets, transforms

# Define the VAE model
class VAE(nn.Module):
    def __init__(self, latent_dim=20):
        super(VAE, self).__init__()
        self.encoder = nn.Sequential(
            nn.Linear(784, 400),
            nn.ReLU(),
        )
        self.fc_mu = nn.Linear(400, latent_dim)
        self.fc_logvar = nn.Linear(400, latent_dim)

        self.decoder = nn.Sequential(
            nn.Linear(latent_dim, 400),
            nn.ReLU(),
            nn.Linear(400, 784),
            nn.Sigmoid()
        )

    def encode(self, x):
        h = self.encoder(x)
        return self.fc_mu(h), self.fc_logvar(h)

    def reparameterize(self, mu, logvar):
        std = torch.exp(0.5*logvar)
        eps = torch.randn_like(std)
        return mu + eps*std

    def decode(self, z):
        return self.decoder(z)

    def forward(self, x):
        mu, logvar = self.encode(x.view(-1, 784))
        z = self.reparameterize(mu, logvar)
        return self.decode(z), mu, logvar

# Loss function
class VAELoss(nn.Module):
    def __init__(self):
        super(VAELoss, self).__init__()
        self.bce_loss = nn.BCELoss(reduction='sum')

    def forward(self, recon_x, x, mu, logvar):
        BCE = self.bce_loss(recon_x, x.view(-1, 784))
        KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())
        return BCE + KLD

# Training function
def train(model, device, train_loader, optimizer, loss_function, epoch):
    model.train()
    for batch_idx, (data, _) in enumerate(train_loader):
        data = data.to(device)
        optimizer.zero_grad()
        recon_batch, mu, logvar = model(data)
        loss = loss_function(recon_batch, data, mu, logvar)
        loss.backward()
        optimizer.step()
        if batch_idx % 100 == 0:
            print(f'Train Epoch: {epoch} [{batch_idx * len(data)}/{len(train_loader.dataset)} '
                  f'({100. * batch_idx / len(train_loader):.0f}%)]\tLoss: {loss.item():.6f}')

# Main training loop
def main():
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    # Load MNIST dataset
    train_loader = DataLoader(
        datasets.MNIST('../data', train=True, download=True,
                       transform=transforms.ToTensor()),
        batch_size=128, shuffle=True)

    model = VAE().to(device)
    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)
    loss_function = VAELoss()

    for epoch in range(1, 11):
        train(model, device, train_loader, optimizer, loss_function, epoch)

if __name__ == '__main__':
    main()
#+end_src

**** byt5
#+begin_src python
  import torch
  import torch.nn as nn
  import torch.nn.functional as F

  class ByT5Encoder(nn.Module):
      def __init__(self, d_model, nhead, num_layers, dim_feedforward):
          super().__init__()
          self.embedding = nn.Embedding(256, d_model)  # 256 possible byte values
          encoder_layer = nn.TransformerEncoderLayer(d_model, nhead, dim_feedforward)
          self.encoder = nn.TransformerEncoder(encoder_layer, num_layers)

      def forward(self, src):
          src = self.embedding(src)
          return self.encoder(src)

  class ByT5Decoder(nn.Module):
      def __init__(self, d_model, nhead, num_layers, dim_feedforward):
          super().__init__()
          self.embedding = nn.Embedding(256, d_model)
          decoder_layer = nn.TransformerDecoderLayer(d_model, nhead, dim_feedforward)
          self.decoder = nn.TransformerDecoder(decoder_layer, num_layers)

      def forward(self, tgt, memory):
          tgt = self.embedding(tgt)
          return self.decoder(tgt, memory)

  class ByT5(nn.Module):
      def __init__(self, d_model=512, nhead=8, num_encoder_layers=6,
                   num_decoder_layers=6, dim_feedforward=2048):
          super().__init__()
          self.encoder = ByT5Encoder(d_model, nhead, num_encoder_layers, dim_feedforward)
          self.decoder = ByT5Decoder(d_model, nhead, num_decoder_layers, dim_feedforward)
          self.output_proj = nn.Linear(d_model, 256)  # Project back to byte space

      def forward(self, src, tgt):
          memory = self.encoder(src)
          output = self.decoder(tgt, memory)
          return self.output_proj(output)

      def encode(self, src):
          return self.encoder(src)

      def decode(self, tgt, memory):
          output = self.decoder(tgt, memory)
          return self.output_proj(output)

  class ByT5Loss(nn.Module):
      def __init__(self, ignore_index=-100):
          super().__init__()
          self.loss_fn = nn.CrossEntropyLoss(ignore_index=ignore_index)

      def forward(self, logits, targets):
          # logits shape: [batch_size, sequence_length, 256]
          # targets shape: [batch_size, sequence_length]
          return self.loss_fn(logits.view(-1, 256), targets.view(-1))

  # convert text to byte tensors
  def text_to_bytes(text):
      return torch.tensor([ord(c) for c in text.encode('utf-8')], dtype=torch.long)

  # Example usage
  model = ByT5()
  src_text = "Hello, world!"
  tgt_text = "Bonjour, monde!"

  src = text_to_bytes(src_text).unsqueeze(0)  # Add batch dimension
  tgt = text_to_bytes(tgt_text).unsqueeze(0)

  output = model(src, tgt)
  print(output.shape)  # Should be [1, tgt_len, 256]
#+end_src

Data loader and main training loop implemented in pytorch
#+begin_src python
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import numpy as np

class ByteTranslationDataset(Dataset):
    def __init__(self, src_texts, tgt_texts):
        self.src_bytes = [self.text_to_bytes(text) for text in src_texts]
        self.tgt_bytes = [self.text_to_bytes(text) for text in tgt_texts]

    def text_to_bytes(self, text):
        return torch.tensor([ord(c) for c in text.encode('utf-8')], dtype=torch.long)

    def __len__(self):
        return len(self.src_bytes)

    def __getitem__(self, idx):
        return {
            'src_bytes': self.src_bytes[idx],
            'tgt_bytes': self.tgt_bytes[idx]
        }

def collate_fn(batch):
    # Pad sequences to the same length within a batch
    src_bytes = [item['src_bytes'] for item in batch]
    tgt_bytes = [item['tgt_bytes'] for item in batch]

    # Pad sequences
    src_bytes = torch.nn.utils.rnn.pad_sequence(src_bytes, batch_first=True, padding_value=0)
    tgt_bytes = torch.nn.utils.rnn.pad_sequence(tgt_bytes, batch_first=True, padding_value=0)

    return {
        'src_bytes': src_bytes,
        'tgt_bytes': tgt_bytes
    }

def train_epoch(model, dataloader, optimizer, loss_fn, device):
    model.train()
    total_loss = 0

    for batch in dataloader:
        # Move data to device
        src_bytes = batch['src_bytes'].to(device)
        tgt_bytes = batch['tgt_bytes'].to(device)

        # Zero the gradients
        optimizer.zero_grad()

        # Forward pass
        # Use teacher forcing during training
        logits = model(src_bytes, tgt_bytes[:, :-1])  # Remove last token for teacher forcing

        # Compute loss
        loss = loss_fn(logits, tgt_bytes[:, 1:])  # Shift target by one for prediction

        # Backward pass
        loss.backward()

        # Gradient clipping
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)

        # Optimizer step
        optimizer.step()

        # Accumulate loss
        total_loss += loss.item()

    return total_loss / len(dataloader)

def main():
    # Hyperparameters
    BATCH_SIZE = 32
    LEARNING_RATE = 1e-4
    NUM_EPOCHS = 10
    D_MODEL = 512
    NHEAD = 8
    NUM_ENCODER_LAYERS = 6
    NUM_DECODER_LAYERS = 6
    DIM_FEEDFORWARD = 2048

    # Device configuration
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

    # Create sample data
    src_texts = [
        "Hello world",
        "Machine learning is fascinating",
        "Natural language processing",
    ]
    tgt_texts = [
        "Bonjour monde",
        "L'apprentissage automatique est fascinant",
        "Traitement du langage naturel",
    ]

    # Create dataset and dataloader
    dataset = ByteTranslationDataset(src_texts, tgt_texts)
    dataloader = DataLoader(
        dataset,
        batch_size=BATCH_SIZE,
        shuffle=True,
        collate_fn=collate_fn
    )

    # Initialize model
    model = ByT5(
        d_model=D_MODEL,
        nhead=NHEAD,
        num_encoder_layers=NUM_ENCODER_LAYERS,
        num_decoder_layers=NUM_DECODER_LAYERS,
        dim_feedforward=DIM_FEEDFORWARD
    ).to(device)

    # Loss function
    loss_fn = ByT5Loss().to(device)

    # Optimizer
    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)

    # Learning rate scheduler
    lr_scheduler = optim.lr_scheduler.ReduceLROnPlateau(
        optimizer,
        mode='min',
        factor=0.5,
        patience=3
    )

    # Training loop
    for epoch in range(NUM_EPOCHS):
        train_loss = train_epoch(model, dataloader, optimizer, loss_fn, device)

        print(f"Epoch {epoch+1}/{NUM_EPOCHS}")
        print(f"Training Loss: {train_loss:.4f}")

        # Update learning rate
        lr_scheduler.step(train_loss)

    # Save the model
    torch.save(model.state_dict(), 'byt5_model.pth')

if __name__ == '__main__':
    main()
#+end_src
*** Latent Sampling

Random Sampling:
#+begin_src python
  def sample_latent_space(model, num_samples):
      # Sample from standard normal distribution
      z = torch.randn(num_samples, model.latent_dim)

      # Optionally, pass through decoder to generate samples
      with torch.no_grad():
          reconstructed_samples = model.decoder(z)

      return reconstructed_samples
#+end_src


Interpolation Sampling:
#+begin_src python
  def interpolate_latent_space(model, z1, z2, num_steps=10):
      # Linear interpolation between two points in latent space
      alphas = torch.linspace(0, 1, num_steps)
      interpolated_samples = []

      with torch.no_grad():
          for alpha in alphas:
              z_interp = (1 - alpha) * z1 + alpha * z2
              sample = model.decoder(z_interp)
              interpolated_samples.append(sample)

      return torch.stack(interpolated_samples)
#+end_src

Visualizing Disentangled Clusters
#+begin_src python
  import umap
  import matplotlib.pyplot as plt
  import seaborn as sns

  def visualize_latent_space(model, dataloader):
      # Collect latent representations
      latent_reps = []
      labels = []

      with torch.no_grad():
          for batch, label in dataloader:
              # Get mu from encoder
              mu, _ = model.encoder(batch)
              latent_reps.append(mu)
              labels.append(label)

      # Concatenate and reduce dimensionality
      latent_reps = torch.cat(latent_reps)
      labels = torch.cat(labels)

      # Use UMAP for dimensionality reduction
      reducer = umap.UMAP(n_components=2)
      reduced_reps = reducer.fit_transform(latent_reps.cpu().numpy())

      # Plot
      plt.figure(figsize=(10, 8))
      scatter = plt.scatter(reduced_reps[:, 0], reduced_reps[:, 1],
                            c=labels, cmap='viridis')
      plt.colorbar(scatter)
      plt.title('Latent Space Visualization')
      plt.show()
#+end_src


  For cluster identification use
  K-Means clustering
  DBSCAN
  Gaussian Mixture Models

Cluster Identification and Sampling
#+begin_src python
  from sklearn.cluster import KMeans

  def identify_and_sample_clusters(model, latent_reps, n_clusters=5):
      # Cluster latent representations
      kmeans = KMeans(n_clusters=n_clusters)
      cluster_labels = kmeans.fit_predict(latent_reps.cpu().numpy())

      # Get cluster centroids
      cluster_centroids = torch.tensor(kmeans.cluster_centers_, dtype=torch.float32)

      # Sample from each cluster
      cluster_samples = []
      with torch.no_grad():
          for centroid in cluster_centroids:
              # Reconstruct from cluster centroid
              sample = model.decoder(centroid.unsqueeze(0))
              cluster_samples.append(sample)

      return cluster_samples, cluster_labels
#+end_src

Traversing Latent Dimensions
#+begin_src python
    pythonCopydef traverse_latent_dimension(model, base_sample, dim_index, num_steps=10):
        # Create copies of base sample, varying one dimension
        traversal_samples = []
        std_range = torch.linspace(-3, 3, num_steps)

        with torch.no_grad():
            for std in std_range:
                # Create a copy of base sample and modify specific dimension
                traversal_sample = base_sample.clone()
                traversal_sample[:, dim_index] = std

                # Reconstruct
                reconstructed = model.decoder(traversal_sample)
                traversal_samples.append(reconstructed)

        return torch.stack(traversal_samples)
#+end_src
